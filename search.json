[
  {
    "objectID": "posts/2021-01-20--Basics-Bayes-ML.html",
    "href": "posts/2021-01-20--Basics-Bayes-ML.html",
    "title": "The Basics of Bayesian Machine Learning",
    "section": "",
    "text": "If you’ve ever been dazzled, or perhaps terrified, by dense integral equations and perplexing terms such as marginal log likelihood and posterior inference when reading a new research paper, you are certainly not alone. For the uninitiated, Bayesian methods appear relegated to austere math-magicians at the heights of the ivory tower conjuring new results with arcane techniques. Fortunately, the fundamentals of Bayesian machine learning are much simpler than they appear and rightly so lest we forget their origins in 18th century Parisian gambling halls at the hands of one Pierre-Simon, marquis de Laplace. In this short post, I’ll outline Bayes’ theorem and how it lies at the heart of all methods in Bayesian machine learning. Internalizing and understanding this basic theorem is all that is required to become a practitioner of Bayesian methods. I will also touch upon some major problems of these methods such as prior elicitation and intractable posteriors. To conclude I will point to some open problems of interest in the field and point to more complete resources for mastering Bayesian machine learning.\n\nReviewing the Basics\nComplete courses in statistics, probability, linear algebra, and calculus are required indeed to fully understand and appreciate the nuance and elegance of Bayesian methods, but the basic laws of probability theory are really all that’s needed to approach Bayes’ theorem. We’ll review them briefly below.\nThe Product Law allows us to compute the probability of two events occuring concurrently:\n\\[Pr(A \\cap B) = Pr(B | A) \\cdot Pr(A)\\]\nNote the term \\(Pr(B | A)\\) in the equation above. This can be read as “the probability of \\(B\\) given \\(A\\)” and is called a conditional probability. As we shall soon see, Bayes’ Theorem is a formulation of conditional probability.\nThe Sum Law allows us to compute the probability of one event if we know the joint probabilities between it and another event:\n\\[Pr(A) = \\sum_{i} Pr(A \\cap B_i)\\]\nWe assume that the events \\(B\\) can be divided into partitions where we know the joint probability of \\(A\\) and the partition \\(B\\). For example, suppose \\(Pr(A)\\) is the probability of flipping heads on a two-sided coin and \\(Pr(B)\\) is the probability of me using a fair coin. Let \\(Pr(B_1)\\) be the probability that the coin is fair and \\(Pr(B_2)\\) be the probability that the coin is unfair. Thus, we could compute the unknown \\(Pr(A)\\) as \\(Pr(A \\cap B_1) + Pr(A \\cap B_2)\\) assuming that we know the joint probabilities of events \\(A\\) and \\(B\\), or, more simply, assuming we know the effect of an unfair coin on the probability of flipping heads. You may see how we can generalize the Sum Law to handle continuous cases by integrating over the partitions of \\(B\\) rather than summing.\n\n\nBayes’ Theorem\nWith the two laws outlined above, we’re ready to tackle Bayes’ Theorem. Let’s begin with the simplest formulation of the theorem:\n\\[Pr(B|A) = \\frac{Pr(A|B) Pr(B)}{Pr(A)}\\]\nThe numerator is just the Product Law applied to the joint probability of \\(A\\) and \\(B\\). Solving for \\(P(B|A)\\) in the Product Rule yields Bayes’ Theorem above. We will need to go further, however, for this to be useful. In practice, finding \\(Pr(A)\\) is really hard, so it is more common to substitute it with the result from the Sum Law:\n\\[Pr(B|A) = \\frac{Pr(A|B) Pr(B)}{\\sum_{i} Pr(A \\cap B_i)}\\]\nWe should generalize the expression for continuous variables since those will often be the variables provided. So far we have used discrete events \\(A\\) and \\(B\\) to illustrate the results. Let’s introduce continuous variables \\(a\\) and \\(b\\) for our generalized expression:\n\\[p(b|a) = \\frac{p(a|b) p(b)}{\\int p(a|b) p(b) db}\\]\nThe process of integrating \\(p(a|b) p(b)\\) to find \\(p(a)\\) is called marginalizing over the parameter \\(b\\) and is extremely common in practice.\nEach part of Bayes’ Theorem has a specific name and function. The probability \\(p(b|a)\\), which is the result we’re seeking to compute, is the posterior probability or belief. It is posterior from the Latin for after since it is the result we arrive at after calculating with Bayes’ Theorem. The term \\(p(a|b)\\) is the likelihood since it represents the likelihood of \\(a\\) given \\(b\\). The probability of \\(b\\) is the prior probability or belief. We begin the Bayesian process with some idea of what we think the prior \\(p(b)\\) is and after incorporating the new evidence presented in the likelihood \\(p(a|b)\\) we arrive at a new belief of \\(p(b)\\) weighted against the new evidence. Finally, we have the ugly term \\(\\int p(a|b) p(b) db\\) which is our marginal probability or belief. Note that in each definition I suggest probability and belief can be used interchangeably. In the Bayesian context, they most certainly can though this can get you into trouble mathematically elsewhere.\n\n\nCoin Flipping Part Deux\nI referenced coin flipping earlier for demonstrating the Sum Law. Now let’s treat this problem more fully using Bayes’ Theorem. Let \\(p(\\theta)\\) be the probability of observing heads. \\(x\\) is the number of heads we observe after \\(n\\) flips. We want to formulate a posterior probability for \\(\\theta\\) given some likelihood and prior. Likelihood probabilities typically are strongly related to the structure of the problem. Our problem is determining the probability of a discrete event occurring given so many trials. A quick gloss of the appendix of any statistics textbook would lead us to choosing a binomial distribution for modeling this likelihood:\n\\[Pr(x | n, \\theta) = {n \\choose x} \\theta^x (1 - \\theta)^{n-x}\\]\nNow on to the prior. There are really two guiding principles for selecting a prior distribution: 1) domain knowledge and 2) mathematical convenience. If we have some deep knowledge about the problem, then we could encode that information into our posterior belief by way of the prior belief. For example, if I truly believed the coin was fair, I may just set \\(p(\\theta) = 0.5\\) and call it a day. Of course this would mean that if the coin was unfair I may not be able realize this in my posterior belief. Therefore, selecting a distribution is more appropriate. Which distribution should I select? Again, maybe prior knowledge of the problem will help you here. Perhaps we think the fairness of the coin is Gaussian distributed, or, for some odd reason we may believe it’s Gamma distributed. In practice though we want a prior distribution that will not make our life difficult. We Bayesians are lazy. Remember that we will soon have to solve \\(\\int p(a|b) p(b) db\\), so let’s choose a distribution that will make this easy for us. Prior distributions that play well with their likelihood companion are conjugate priors. Here is a great Wikipedia page on conjugacy that includes a table of common conjugate priors. The prior distribution conjugate to the Binomial distribution is the Beta distribution:\n\\[p(\\theta | \\alpha, \\beta) = \\frac{1}{\\int^1_0 \\theta^{\\alpha - 1} (1- \\theta)^{\\beta-1}d\\theta} \\theta^{\\alpha -1} (1-\\theta)^{\\beta-1}\\]\nThe equation above may look messy, but we should just understand it as the continuous analog of the Binomial distribution.\nWith our likelihood and prior beliefs in hand, we can now formulate our posterior belief of \\(\\theta\\):\n\\[p(\\theta| x, n, \\alpha, \\beta) = \\frac{Pr(x | n, \\theta) p(\\theta | \\alpha, \\beta)}{\\int Pr(x | n, \\theta) p(\\theta | \\alpha, \\beta) d\\theta}\\]\nThe algebra for reducing the above is unfortunately pretty hairy but because of our careful selection of a conjugate prior, we know that the result will itself be a Beta distribution. It has the following form:\n\\[p(\\theta| x, n, \\alpha, \\beta) = Beta(\\alpha + x, \\beta + n - x)\\]\nWe’re finished! We’ve modeled the probability of flipping heads as a Beta distribution incorporating the result of trials, the number of trials, and our prior beliefs. You may wonder at this point what we should set \\(\\alpha\\) and \\(\\beta\\) to in this model. Again, this relies on prior beliefs. We can set both to 1 which will have the result of saying each possible value of \\(\\theta\\) is equally likely. We may set \\(\\alpha = 500\\) and \\(\\beta = 1000\\) which would indicate that we’re pretty confident that the coin is fair. We can even make the problem more complex and place priors on our priors, which are called hyperpriors. This process of selecting priors is called prior elicitation and is a key problem in Bayesian machine learning. If we select good priors, the model will converge much faster than traditionally methods. If we are egregiously wrong it will take much longer to reach convergence. We must balance picking mathematically convenient priors with priors that reflect the problem fairly. In the case of coin flipping, this is relatively easy. In machine learning this proves to be far more difficult.\n\n\nReasoning With The Posterior\nThis may all seem like a lot of work. Why can’t we just work with the likelihood directly and call it a day? That is essentially what we do in most other machine learning methods that rely on maximum likelihood estimation (MLE) for parameter fitting, but it does have one big limitation. The results of MLE methods are not probabilities, they are point estimates. This unfortunately means we cannot quantify our uncertainty of the prediction rigorously which can be very problematic. At the end of the day, machine learning is about automated decision making. We humans weigh our decisions by the certainty we have in the evidence. If you believe that it will rain tomorrow because the weather man was right yesterday, your belief that it will rain the day after that will inevitably be weighted by whether the weather will be right tomorrow. Our models should follow the same principles to make the right decisions. Luckily, Bayesian methods make this very easy for us. A posterior belief is much more powerful than a point estimate since it is represented as a probability distribution. Consider the posterior belief from the previous section \\(p(\\theta| x, n, \\alpha, \\beta)\\). If we wanted to compute a 95% credible interval for this belief, meaning construct some interval where it is 95% likely for the parameter \\(\\theta\\) to lie, we simply find bounds where the following is true\n\\[0.95 = Pr(\\theta \\in (a, b)| x, n, \\alpha, \\beta) = \\int^a_b p(\\theta| x, n, \\alpha, \\beta) d\\theta\\]\nSimilarly, we can hypothesis test our posterior belief by working with the same integral representation. We can formulate the hypothesis that \\(\\theta=0.5\\), i.e. the coin is fair, as\n\\[p(\\theta = 0.5| x, n, \\alpha, \\beta) = \\int^{0.5}_{0.5} p(\\theta = 0.5| x, n, \\alpha, \\beta) d\\theta\\]\nThe result of the hypothesis test above is more useful than its Frequentist counterpart since we receive a probability of the hypothesis being true rather than a mere indication to accept or reject.\nClearly the posterior belief has a central role in Bayesian machine learning, and when it takes form of a known distribution like the Beta distribution, integrating it and using it for predictions is trivial. Our world though is more complex than what the pages of a mathematical appendix can describe. We are often left working with intractable posteriors when our likelihood and prior are not conjugate. For example, in Bayesian logistic regression the Bernoulli likelihood and Gaussian prior lead us to a posterior we cannot express analytically. We are not totally at a loss however. Instead, we can use methods of approximation. Often times we’re not interested in all the details of a posterior belief. Again considering Bayesian logistic regression, we really just need the expectation of the posterior to perform inference. A common trick in this case is to use the Laplace Approximation which superimposes a Gaussian over the intractable distribution and sets its mean to the mode of the intractable. Other methods include Expectation Propagation and Variational Inference that are more common in complex models like Gaussian processes and Deep Bayes Nets. We can also use traditional computation methods like Markov Chain Monte Carlo for approximating posteriors though in practice those are too intense to include in a training loop for machine learning.\n\n\nKnown Unknowns\nBayesian machine learning is a large and growing field. Some areas of active research naturally follow some problems we discussed above such as how to pick good priors and how to deal with tricky posteriors. Others areas aim to Bayes-ify results from other fields. Simply google “Bayesian method-of-choice” and be inundated with the likes of Bayesian LSTMs, Bayesian PCA, Bayesian decision trees etc. It has been extremely useful for statisticians and scientists to be able to quantify how much they do not know, and as machine learning continues to share the brunt of decision-making, working with uncertainty will only become more important."
  },
  {
    "objectID": "posts/2021-05-05-laplace.html",
    "href": "posts/2021-05-05-laplace.html",
    "title": "Laplace Approximation",
    "section": "",
    "text": "Many algorithms in the modern machine learning toolbox have had full lives before being canonized in familiar textbooks, and their stories and historical applications are often as interesting and enlightening as their current role in the field. The Laplace approximation is certainly one of those. Today it is an elegantly simple method for approximating posterior distributions that are roughly Gaussian in nature, such as the posterior of the Gaussian process classifier model. That application alone has granted it a place in scikit-learn. It has also been successfully applied to estimating the uncertainty of arbitrary deep learning models without the need to retrain as seen in this 2021 DeepMind paper. But the Laplace approximation wasn’t specifically invented for any of these applications. Its history begins over 200 hundred years ago when Pierre Laplace published a series of seminal texts on the “science of chances.” It later was used in approximating factorials and eventually found itself where it is today. I personally first encountered this method in graduate school and have since been enamored by its simplicity. This blog post will illustrate how exactly this technique functions and some of its interesting applications so that you the reader may have better context when reading sklearn documentation or Bayesian deep learning papers. To that end, we’ll begin with a motivating example of how the Laplace approximation is used most often today before doing a deep dive in the mathematics behind it. We’ll then take a brief detour to see how we can apply this technique to the very different problem of approximating factorials before returning to our initial example with our newfound knowledge of how to solve it."
  },
  {
    "objectID": "posts/2021-05-05-laplace.html#problematic-posteriors",
    "href": "posts/2021-05-05-laplace.html#problematic-posteriors",
    "title": "Laplace Approximation",
    "section": "Problematic Posteriors",
    "text": "Problematic Posteriors\nSuppose we would like to use logistic regression in an instance where uncertainty estimates are crucial, e.g., a binary classifier of radiological data that determines whether a tumor is benign or cancerous. A positive result indicating that a tumor is in fact cancerous is a serious affair, so we should have an understanding as to how certain we are about the diagnosis. We can use Bayes’ Theorem to formulate this problem appropriately. Logistic regression is a parametric model for classification, so our prior belief should relate to the weights and our likelihood should define the relationship between the inputs and outputs of our model. A reasonable prior belief for our weights is that they are Gaussian distributed. Since this is a binary classification problem, we want to constrain our output space to just 0 and 1 which means that both Bernoulli and Binomial distributions are candidates for our likelihood. No matter our choice, however, our posterior belief will be intractable since Gaussian priors are not conjugate with either Bernoulli or Binomial distributions.\nSuppose that after feeding our data to this logistic model we draw samples from the posterior which looks like this:\n\n\nText(0, 0.5, 'Probability')\n\n\n\n\n\nThe true closed form solution for the posterior1 above is\n\np(z) = \\exp(\\frac{-z^2}{2}) \\cdot (1 + \\exp(-20z-4))^{-1}\n\nUnfortunately we will very rarely have direct access to the closed form solution for the posterior, so we must approximate. Enter: Pierre-Simon Laplace."
  },
  {
    "objectID": "posts/2021-05-05-laplace.html#a-solution-from-parisian-dice-parlors",
    "href": "posts/2021-05-05-laplace.html#a-solution-from-parisian-dice-parlors",
    "title": "Laplace Approximation",
    "section": "A Solution from Parisian Dice Parlors",
    "text": "A Solution from Parisian Dice Parlors\nThe Marquis de Laplace was a busy man. From his service to one Napoleon Bonaparte to his time frequenting gambling dens, there was no shortage of opportunities where Laplace needed to estimate the odds of an event. His contributions to the field of probability are almost unmatched: he rigorously formulated and popularized Bayes’ Theorem, discovered probability generating functions, tied the least squares method to probability, and proved the central limit theorem which underpins the fields of statistics and probability today. But right now we are mostly concerned with a method developed in his 1774 manuscript Mémoires de Mathématique et de Physique.\n\nDeriving the Laplace Approximation\nThe Laplace Approximation is a very general method for working with integrals of the form \n\\int^a_b \\exp(M \\cdot f(x)) dx\n\nwhere M is some large number and f(x) is at least twice differentiable.\nThe basic idea is that we can use a Taylor series around the global maximum of the function as an approximation of the original intractable integral. Suppose that f(x) has a global maximum around x_0 and that x_0 is not an endpoint. Then, using the first few terms of the Taylor series we have\n\nf(x) = f(x_0) + f'(x_0)(x-x_0) + \\frac{1}{2}f''(x_0)(x-x_0)^2\n\nKnowing that f(x_0) is a global maximum allows us to simplify a bit further since f'(x_0) should be 0.\n\nf(x) \\approx f(x_0) - \\frac{1}{2}|f''(x_0)|(x-x_0)^2\n\nIf we plug this back into our original integral equation we have \n\\int^a_b \\exp(M \\cdot f(x)) dx \\approx \\int^a_b \\exp(M \\cdot f(x_0) - \\frac{1}{2}|f''(x_0)|(x-x_0)^2) dx\n\nwhich further simplifies to \n\\exp(M f(x_0)) \\cdot \\int^a_b \\exp(- \\frac{1}{2}|f''(x_0)|(x-x_0)^2) dx\n\nRecall that the Gaussian integral has the form:\n\n\\int \\exp(-x)^2 dx = \\sqrt{\\pi}\n\nAssuming that the integral we’re interested in is an indefinite integral as is the case with most probability density functions, the Laplace Approximation allows us to use the Gaussian integral to approximate this sort of intractable integral. Using the Gaussian integral, we can reduce the above equation into the final formulation of Laplace Approximation:\n\n\\int^a_b \\exp(M \\cdot f(x)) dx \\approx \\sqrt{\\frac{2 \\pi}{M |f''(x_0)|}}\\exp(Mf(x_0))\n\nThe accuracy of this approximation improves as M \\rightarrow \\infty.\n\n\nA Simple Application\nLet’s consider an application of the Laplace Approximation to a well-known problem in computer science. Suppose you wish to write a program to quickly compute f(n) = n!. Factorial equations are used all throughout physics, probability, and mathematics with practical uses in computing Bézier curves for computer graphics and time complexity equations for sorting algorithms.\nA naive solution may look like:\n\ndef factorial(n):\n    result = 1\n    for i in range(1, n+1):\n        result *= i\n    return result\n\nfactorial(5)\n\n120\n\n\nIf we needed to run this function on very large integers thousands of times a second which may often be the case especially in computer graphics, then we need this code to be especially fast. Multiplying increasing large integers in a for loop will probably become a bottleneck rather quickly, so this is a good opportunity to apply an approximation. Let’s begin with the similar and useful Gamma function:\n\n\\Gamma(n) = (n-1)! = \\int_0^{\\infty} \\exp(-x) x^{n-1} dx\n\nOur factorial equation can then be written as\n\nn! = \\Gamma(n + 1) = \\int_0^{\\infty} \\exp(-x)x^ndx\n\nBy performing a change of variables so that x = nz and dx = n dz we can derive the following\n\n= \\int_0^{\\infty} \\exp(-nz) (nz)^n n dz \\\\\n= n^{n+1} \\int_0^{\\infty} \\exp(-nz)z^n dz \\\\\n= n^{n+1} \\int_0^{\\infty} \\exp(-nz) \\exp(n \\ln z) dz \\\\\n= n^{n+1} \\int_0^{\\infty} \\exp(n (\\ln z - z)) dz\n\nThe final integral above has the form required for the Laplace Approximation: \\int^a_b \\exp(M \\cdot f(x)) dx since f(z) = \\ln z - z is twice differentiable. Recall that as M \\rightarrow \\infty, the approximation is more accurate so our final approximation of the factorial equation above will be more accurate for larger choices of n.\nUsing the Laplace approximation, we arrive at Stirling’s Approximation for computing factorials:\n\n\\int^a_b \\exp(M \\cdot f(x)) dx \\approx \\sqrt{\\frac{2 \\pi}{M |f''(x_0)|)}}\\exp(Mf(x_0))\\\\\nn^{n+1} \\int_0^{\\infty} \\exp(n (\\ln z - z)) dz \\approx n^{n+1} \\sqrt{\\frac{2 \\pi}{n |f''(x_0)|}}\\exp(n f(x_0))\n\nf(z) = \\ln z - z, so f'(z) = \\frac{1}{z} - 1 and f''(z) = -\\frac{1}{z^2}. The max of f(z) is found at z=1, so f(z_0=1) = -1 and f''(z_0 = 1) = -1 as well. With this we arrive at the final approximation:\n\nn! \\approx n^{n+1} \\sqrt{\\frac{2 \\pi}{n}}e^{-n}\n\nwhich in Python would be\n\ndef stirling_approx(n):\n    return (n ** (n+1)) * (np.sqrt((2 * np.pi) / n)) * np.exp(-n)\n\nstirling_approx(5)\n\n118.01916795759008\n\n\nThe percent error between the actual factorial compuation and approximation in this example is\n\nprint(100 * round((factorial(5) - stirling_approx(5)) / factorial(5), 2), \"%\")\n\n2.0 %\n\n\nNote that as n \\rightarrow \\infty the percent error approaches 0 for this approximation."
  },
  {
    "objectID": "posts/2021-05-05-laplace.html#returning-to-uncertainty",
    "href": "posts/2021-05-05-laplace.html#returning-to-uncertainty",
    "title": "Laplace Approximation",
    "section": "Returning to Uncertainty",
    "text": "Returning to Uncertainty\nNow that we’ve covered the mechanics of the Laplace Approximation and applied it to the simple factorial problem, let’s return to our original problem and approximate that tricky posterior.\nWe need to know 1) at which point the probability density function is maximized, i.e., the mode of the posterior and then 2) we need to evaluate that point with the second derivative of the probability density function. If we were approximating a multivariate distribution, this then becomes slightly more complex by requiring us to determine the Hessian matrix of the posterior.\nFinding the mode of a distribution is typically straightforward. If we are working in a model context like Bayesian logistic regression, then we can typically compute the maximum a posteriori (MAP) estimate directly. Otherwise, we can sample the distribution to determine the mode. Note, however, that distributions of higher dimensions cannot be sampled efficiently so it is generally ideal to work with algorithms where MAP estimations are trivial.\nSecond derivatives and Hessians unfortunately were historically difficult to determine numerically, but thankfully this is changing. When working with well-studied algorithms like Bayesian logistic regression, it is usually possible to find the necessary derivatives from the loss function either directly or with a numerical techniques like Newton’s method. These tend to be quite compute expensive, but there have been a lot of recent advances that make this process much more efficient.\nLet’s take another look at that tricky posterior from earlier:\n\n\n<matplotlib.legend.Legend at 0x12e95eb20>\n\n\n\n\n\nWe essentially want to find a PDF q(x) to approximate our unknown p(x) above. In the context of continuous probability distributions, we typically prefer to work with the negative log likelihood since it is tends to be easier to optimize than the original likelihood function. Let’s revisit our derivation of the Laplace Approximation to consider the log likelihood \\ln p(x). The Taylor approximation is \n\\ln p(x) \\approx \\ln p(x_0) -\\frac{1}{2} (-p''(x_0)) (x-x_0)^2\n Exponetiating the above leads to \np(x) \\approx p(x_0) \\exp(-\\frac{p''(x_0)}{2}(x-x_0)^2)\n Again recognizing that \\exp(-\\frac{p''(x_0)}{2}(x-x_0)^2) is essentially an unnormalized Gaussian PDF, we can formulate the normalized Gaussian q(x) that we can use to approximate our posterior: \nq(x) = \\sqrt{\\frac{-p''(x_0)}{2\\pi}} \\exp(-\\frac{p''(x_0)}{2}(x-x_0)^2)\n\nNow we just need to compute the mode of the posterior x_0 and the term p''(x_0). Below we compute the mode by sampling the distribution. Note that this may not always be possible, but in cases where the mode or p''(x_0) cannot be found by sampling we often have direct access to the loss function or posterior PDF from which can we derive a closed form solution for the Laplace approximation.\n\nsample_interval = np.linspace(-2, 4, 1000)\nposterior_samples = p(sample_interval)\nmax_point = sample_interval[np.argmax(posterior_samples)]\n\nNow we approximate p''(x_0). For this example, we’ll just use scipy.misc.derivative which is an implementation of the central difference equation for approximation. Again this assumes we can sample the posterior directly.\n\nfrom scipy.misc import derivative\n\ndef ln_p(x): return np.log(p(x))\n\napprox_sec_deriv = -1 * derivative(ln_p, max_point, dx=1e-6, n=2)\nprint(\"p''(x_0) \", round(approx_sec_deriv, 4))\n\np''(x_0)  2.5253\n\n\nWe can finally plot our approximated Gaussian against the original posterior.\n\n\n<matplotlib.legend.Legend at 0x108d33580>\n\n\n\n\n\nAs we can see, the approximated posterior is very close to the actual distribution.\nWe should note the flaws with this method though. Some of our approximations are quite expensive. Numerically determining the second derivative with the central difference equation for a single variable once may not be a big deal, but this will quickly become infeasible as we increase the dimensionality of the distribution. Additionally, if this was part of a larger model training loop, approximating a Hessian matrix every batch would be prohibatively expensive. For the multidimensional case where approximating a Hessian matrix is necessary, recent advances such as Kronecker-factored approximate curvature (KFAC) make the Laplace approximation much more tractable to the point where it can be used in modern deep learning pipelines for general uncertainty estimation as it is in this recent paper.\nBeyond computational concerns, the underlying assumptions of the Laplace approximation may not be approriate for all problems. It is assumed that the true posterior is roughly Gaussian, but if it is in fact a heavily skewed Weibull distribution for example, then the approximated Gaussian will probably not be appropriate. Note too that the Laplace approximation puts the most emphasis on estimating the mode of the true distribution. If you are more interested in accurately capturing its variance instead, then other approximation techniques may be more appropriate.\nThe Laplace approximation is a simple but effective technique for approximating tricky posterior distributions. Though not perfect, it is nonetheless an important tool to have in the toolbox especially when working with Bayesian machine learning techniques. From the Napoleonic battlefield to the modern deep learning pipeline, it has continuously proved its usefulness."
  },
  {
    "objectID": "papers.html",
    "href": "papers.html",
    "title": "Papers and Talks",
    "section": "",
    "text": "Client Side Deep Learning Optimization with PyTorch\n\nPresented at Strange Loop 2021\n\n\nAbstract: Deep learning has the capacity to take in rich, high dimensional data and produce insights that can create totally new mobile experiences for developers. However, the constraints of network availability and latency limit what kinds of work can be done in the mobile application space and vastly increase the cost to developers. We have recently developed a customer facing mobile application that leverages real-time computer vision models and will share our experiences of moving multiple deep learning models from the server onto the client. In this presentation, we dive into technical solutions for porting custom architectures for various vision tasks and how to serialize them from Python to binary assets, while avoiding common issues such as unsupported hardware instructions. We also discuss the theory and practice of quantizing models, model fusion, and storing tensors in last memory format for optimization. We conclude by demonstrating how to benchmark the performance of client-side models for various devices and operating systems.\n\n\n\nAuceps syllabarum: A Digital Analysis of Latin Prose Rhythm\n\nPublished in the Journal of Roman Studies 2019\nKeeline, T., & Kirby, T. (2019). Auceps syllabarum: A Digital Analysis of Latin Prose Rhythm. Journal of Roman Studies, 109, 161-204. doi:10.1017/S0075435819000881\nAbstract: In this article we describe a series of computer algorithms that generate prose rhythm data for any digitised corpus of Latin texts. Using these algorithms, we present prose rhythm data for most major extant Latin prose authors from Cato the Elder through the second century A.D. Next we offer a new approach to determining the statistical significance of such data. We show that, while only some Latin authors adhere to the Ciceronian rhythmic canon, every Latin author is ‘rhythmical’ — they just choose different rhythms. Then we give answers to some particular questions based on our data and statistical approach, focusing on Cicero, Sallust, Tacitus and Pliny the Younger. In addition to providing comprehensive new data on Latin prose rhythm, presenting new results based on that data and confirming certain long-standing beliefs, we hope to make a contribution to a discussion of digital and statistical methodology in the study of Latin prose rhythm and in Classics more generally. The Supplementary Material available online contains an appendix with tables, data and code. This appendix constitutes a static ‘version of record’ for the data presented in this article, but we expect to continue to update our code and data; updates can be found in the repository of the Classical Language Toolkit."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Laplace Approximation\n\n\n\n\n\n\n\nmachine_learning\n\n\n\n\nBayesian machine learning often requires working with posterior distributions that have no known closed form. There are plenty of approximation methods, though many tend to be difficult to understand and implement or too computationally expensive. This post provides an overview of the Laplace Approximation, an exceptionally simple approach that is seeing a resurgence in recent deep learning papers.\n\n\n\n\n\n\nDec 5, 2022\n\n\n\n\n\n\n\n\nThe Basics of Bayesian Machine Learning\n\n\n\n\n\n\n\nmachine_learning\n\n\n\n\nBayesian methods can offer capabilities like uncertainty estimates and encoding domain knowledge directly into a model. This post provides an overview of Bayesian methods beginning with a review of probability before showing how it can be applied to the coin flipping problem. By the end, the reader will have a basic understanding of the methods and where to go from here.\n\n\n\n\n\n\nJan 20, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’ve been working in data science for about firve years now, and previously studied classical lanaguages and literature. When I’m not thinking about ways to improve the world with machine learning or why Odysseus couldn’t help but taunt the cyclops, I can be found reading widely, cooking extensively and overburdening myself with yet more musical instruments.\nI live in St. Louis, Missouri with my wife and two happy dachshunds."
  }
]